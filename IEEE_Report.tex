% IEEE Conference Paper Format - Billboard Replacement Pipeline
% Compile with pdflatex

\documentclass[conference]{IEEEtran}
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{hyperref}

\begin{document}

\title{Automated Billboard Replacement in Videos: A Deep Learning Pipeline for Real-Time Content Localization}

\author{
\IEEEauthorblockN{Emre Belikırık}
\IEEEauthorblockA{Department of Computer Engineering\\
Hacettepe University\\
Ankara, Turkey\\
emrebelikirik25@hacettepe.edu.tr}
}

\maketitle

%=============================================================================
% ABSTRACT
%=============================================================================
\begin{abstract}
Manual billboard replacement in video content is a labor-intensive and expensive post-production process that limits content localization and dynamic advertising capabilities. This paper presents an end-to-end automated pipeline for billboard detection, segmentation, tracking, and replacement in video sequences. The system integrates state-of-the-art deep learning models including fine-tuned YOLOv8 variants for detection and segmentation, SAM2 (Segment Anything Model 2) for video object segmentation, and multiple tracking algorithms for temporal mask propagation. A comprehensive comparison was conducted between fine-tuned models and zero-shot approaches using YOLO-World for open-vocabulary detection. Additionally, various tracking strategies, ranging from optical flow to planar Kalman filtering, were evaluated to analyze trade-offs between accuracy and real-time performance. The proposed system enables automated, scalable billboard content replacement suitable for digital advertising and content localization applications.
\end{abstract}

\begin{IEEEkeywords}
billboard detection, video object segmentation, deep learning, SAM2, YOLOv8, YOLO-World, perspective transformation
\end{IEEEkeywords}

%=============================================================================
% I. INTRODUCTION
%=============================================================================
\section{Introduction}

The digital advertising industry increasingly demands dynamic content that can be localized, personalized, and updated in real-time. Billboards appearing in video content---whether in sports broadcasts, films, or user-generated media---present significant opportunities for targeted advertising and content monetization. However, the manual process of replacing billboard content in video sequences is prohibitively expensive, requiring skilled visual effects artists and extensive frame-by-frame editing.



Traditional post-production workflows face several challenges: (1) the labor-intensive nature of manual segmentation, (2) difficulty maintaining temporal consistency across video frames, (3) handling perspective distortions as camera angles change, and (4) ensuring seamless blending between replaced content and original footage. These challenges motivate the development of automated solutions that can reduce costs while maintaining visual quality.

This work addresses these challenges by developing an end-to-end automated pipeline for billboard replacement in videos. The proposed approach leverages recent advances in deep learning, specifically:

\begin{itemize}
    \item \textbf{Detection:} Fine-tuned YOLOv8-n/s/m object detection models that predict bounding boxes around billboard regions. Additionally, zero-shot YOLO-World is evaluated for open-vocabulary detection without domain-specific training data. Detection provides the initial localization that guides subsequent processing stages.
    \item \textbf{Segmentation:} Pixel-level mask prediction using fine-tuned YOLOv8-seg models that simultaneously predict bounding boxes and instance masks. Furthermore, SAM2 (Segment Anything Model 2) is integrated as a foundation model that accepts detection outputs (bounding boxes or points) as prompts to generate precise segmentation masks. This hybrid approach combines the detection accuracy of fine-tuned models with SAM2's zero-shot segmentation capability.
    \item \textbf{Tracking:} Multiple tracking algorithms including SAM2 propagation with memory-based attention, Kalman filtering for smooth motion prediction, and optical flow methods for temporal mask consistency across video frames.
    \item \textbf{Perspective Transformation:} Automated 4-corner extraction from segmentation masks and homography-based warping for geometrically correct replacement content insertion.
    \item \textbf{Post-Processing:} Temporal smoothing algorithms including Kalman filtering and IIR cascaded filters to eliminate flicker and ensure visual coherence across frames.
\end{itemize}

The main contributions of this paper are:
\begin{enumerate}
    \item A comprehensive comparison of fine-tuned vs. zero-shot detection and segmentation approaches for billboard detection.
    \item An extensive benchmark of six tracking algorithms evaluating accuracy-speed trade-offs.
    \item An integrated pipeline combining detection, tracking, and replacement with temporal smoothing.
\end{enumerate}

%=============================================================================
% II. RELATED WORK
%=============================================================================
\section{Related Work}

The process of automated billboard replacement relies on combining several established computer vision tasks. The most basic requirement is object detection to find where a billboard is in a frame. YOLO (You Only Look Once) \cite{redmon2016yolo} is the most popular choice for this because it is fast and easy to implement. Recent versions like YOLOv8 \cite{ultralytics2021} allow us to not only detect boxes but also get pixel-level masks through instance segmentation, which is a key part of our project.

For more complex shapes, the Segment Anything Model (SAM) \cite{kirillov2023segment} has changed how we think about segmentation. Instead of training a model for every specific object, SAM allows us to find objects by just giving it a box or a point prompt. Our work uses the newer SAM2 \cite{ravi2024sam2} variant, which is designed to handle videos by "remembering" objects from previous frames. This helps solve the problem of flickering masks in video sequences.

Tracking is another area where many methods exist. Simple methods like Optical Flow or Kalman Filters \cite{kalman1960new} are very fast but often fail if the camera moves too quickly. More advanced methods like XMem \cite{cheng2022xmem} or Cutie \cite{cheng2023cutie} use deep learning to keep track of objects over long periods. We compare several of these to find the best balance between speed and accuracy for a real-time system.

Finally, for the actual replacement, most commercial systems use manual editing. Some researchers have started using generative models like Stable Diffusion \cite{rombach2022high} to "paint" new ads into videos. However, for billboards, a simple perspective transformation using the OpenCV library \cite{bradski2000opencv} is usually enough to get a realistic result. Our pipeline follows the inspiration of projects like "Track Anything" \cite{yang2023track}, but adds an automated detection step so the user doesn't have to click on the billboard manually.
%=============================================================================
% III. METHODOLOGY
%=============================================================================
\section{Methodology}

The pipeline consists of four sequential stages: Detection \& Segmentation, Tracking, Replacement, and Post-Processing. Figure \ref{fig:pipeline} illustrates the overall system architecture.

\begin{figure}[htbp]
\centerline{\includegraphics[width=\columnwidth]{system_diagram.png}}
\caption{Overview of the automated billboard replacement pipeline showing the four processing stages.}
\label{fig:pipeline}
\end{figure}

\subsection{Stage 1: Detection and Segmentation}

The first stage localizes billboards in video frames and generates precise segmentation masks. This stage evaluates multiple approaches for both detection (bounding box prediction) and segmentation (pixel-level mask prediction).

\subsubsection{Detection Approaches}

\paragraph{Fine-Tuned YOLOv8 Detection}
YOLOv8 detection variants (nano, small, medium) were fine-tuned on a large-scale billboard detection dataset comprising 3,798 images. The detection models predict bounding boxes and class confidence scores for billboard regions. Training utilized extensive data augmentation including HSV color-space transforms (Hue: $\pm0.015$, Saturation: $\pm0.7$, Value: $\pm0.4$), geometric distortions (rotation $\pm15^\circ$, shear $\pm5^\circ$, perspective $0.001$), and advanced regularization (Mosaic $p=1.0$, MixUp $p=0.15$, Random Erasing $p=0.4$). The detection loss combines classification and bounding box regression:

\begin{equation}
\mathcal{L}_{det} = \mathcal{L}_{cls} + \lambda_{box}\mathcal{L}_{box}
\end{equation}

\paragraph{Zero-Shot YOLO-World}
For scenarios without domain-specific training data, YOLO-Worldv2 is employed for open-vocabulary detection. The text prompt ``billboard'' guides the model to detect relevant objects without explicit training on billboard data. While zero-shot approaches offer deployment flexibility, experiments show accuracy gaps compared to fine-tuned models (see Section IV).

\subsubsection{Segmentation Approaches}

\paragraph{Fine-Tuned YOLOv8 Segmentation}
YOLOv8-seg extends the detection architecture with a segmentation head that produces pixel-level instance masks. These variants (nano, small, medium) were fine-tuned on a dedicated segmentation dataset of 485 images with precise polygon annotations. The segmentation output provides pixel-wise masks directly suitable for replacement. The loss function combines classification, bounding box regression, and mask prediction terms:

\begin{equation}
\mathcal{L}_{seg} = \mathcal{L}_{cls} + \lambda_{box}\mathcal{L}_{box} + \lambda_{mask}\mathcal{L}_{mask}
\end{equation}

\paragraph{Mask R-CNN}
As a two-stage alternative, Mask R-CNN first generates region proposals using a Region Proposal Network (RPN), then performs classification, bounding box refinement, and mask prediction for each proposal. This architecture typically produces higher-quality mask boundaries at the cost of increased computational overhead.

\subsubsection{Hybrid Detection + SAM2 Segmentation}

A hybrid pipeline combines the detection accuracy of fine-tuned YOLOv8 models with SAM2's (Segment Anything Model 2) powerful zero-shot segmentation capabilities. In this approach:

\begin{enumerate}
    \item \textbf{Detection Phase:} YOLOv8-det predicts bounding boxes for billboard regions with high precision.
    \item \textbf{Prompt Generation:} Detected bounding boxes are converted to SAM2 prompts (box prompts or center point prompts).
    \item \textbf{SAM2 Segmentation:} SAM2's image encoder processes the frame, and the mask decoder generates precise segmentation masks conditioned on the detection prompts.
\end{enumerate}

SAM2 employs a Vision Transformer (ViT) backbone for image encoding and a lightweight mask decoder with cross-attention mechanisms. The model supports multiple prompting strategies:

\begin{itemize}
    \item \textbf{Box Prompts:} Detected bounding boxes directly guide mask generation.
    \item \textbf{Point Prompts:} Center points of detections provide coarse object location.
    \item \textbf{Mask Prompts:} Previous frame masks can refine current predictions for video applications.
\end{itemize}

This hybrid approach leverages SAM2's ability to produce high-quality masks from minimal prompts while benefiting from domain-specific detection training. Importantly, SAM2's video object segmentation mode enables mask propagation across frames using a memory attention mechanism, which is utilized in the tracking stage (Section III-B) for temporal consistency.

\subsection{Stage 2: Tracking}

Temporal consistency is critical for video applications. Rather than processing each frame independently, the initial segmentation mask is propagated through subsequent frames using tracking algorithms. The system implements an adaptive detection-tracking balance:

\begin{equation}
\text{Mode}_t = \begin{cases}
\text{Detect} & C_t < \tau \text{ or } t \bmod K = 0 \\
\text{Track} & \text{otherwise}
\end{cases}
\end{equation}

where $C_t$ is the tracking confidence at frame $t$, $\tau_{low}$ is the confidence threshold (0.5), and $I_{key}$ is the keyframe interval (30 frames).

\subsubsection{Tracker Implementations}

Six tracking approaches are evaluated:

\begin{itemize}
    \item \textbf{SAM2Tracker:} Combines SAM2's promptable segmentation with a custom temporal history mechanism. It utilizes multiple prompting strategies, including bounding boxes and point sampling from previous masks, while incorporating an optical flow-based motion prediction fallback to maintain mask continuity during detection failures.
    \item \textbf{HybridFlowTracker:} Combines dense optical flow with feature-based matching for robust correspondence.
    \item \textbf{PlanarKalmanTracker:} Models billboard corners as a 16-dimensional state vector $\mathbf{x} = [x_1, y_1, ..., x_4, y_4, \dot{x}_1, \dot{y}_1, ..., \dot{x}_4, \dot{y}_4]^T$ with constant velocity dynamics.
    \item \textbf{FeatureHomographyTracker:} SIFT/ORB feature matching with RANSAC-based homography estimation.
    \item \textbf{ECCHomographyTracker:} Enhanced Correlation Coefficient maximization for sub-pixel registration accuracy.
    \item \textbf{AdaptiveOpticalFlowTracker:} Lucas-Kanade optical flow with forward-backward error checking for point validation.
\end{itemize}

\subsubsection{Sanity Checking System}

Each tracked mask undergoes multi-level validation before acceptance:

\begin{enumerate}
    \item \textbf{Geometric checks:} Convexity (solidity $> 0.7$), aspect ratio bounds
    \item \textbf{Temporal checks:} Area change $< 50\%$ per frame, center movement threshold
    \item \textbf{Consistency checks:} Similarity with recent mask history using IoU
\end{enumerate}

\subsection{Stage 3: Replacement}

Given a validated segmentation mask, we extract the billboard quadrilateral and warp the replacement image using perspective transformation.

\subsubsection{Multi-Strategy Corner Extraction}

Adaptive corner detection is employed:

\begin{enumerate}
    \item \textbf{Polygon approximation:} Using Douglas-Peucker algorithm with adaptive $\epsilon$
    \item \textbf{Minimum area rectangle:} Fallback for non-polygonal masks
    \item \textbf{Extreme point detection:} Project convex hull points in 8 directions, cluster to 4 corners
    \item \textbf{Sub-pixel refinement:} Apply \texttt{cornerSubPix} for precise localization
\end{enumerate}

\subsubsection{Perspective Transformation}

The homography matrix $H$ is computed from source corners (replacement image) to destination corners (billboard region):

\begin{equation}
\begin{bmatrix} x' \\ y' \\ 1 \end{bmatrix} = H \begin{bmatrix} x \\ y \\ 1 \end{bmatrix}, \quad H = \begin{bmatrix} h_{11} & h_{12} & h_{13} \\ h_{21} & h_{22} & h_{23} \\ h_{31} & h_{32} & 1 \end{bmatrix}
\end{equation}

\subsubsection{Advanced Blending Techniques}

Three blending strategies are implemented:

\textbf{Distance-Transform Feathering:} Creates smooth alpha masks based on distance from edges:
\begin{equation}
\alpha(p) = \min\left(1, \frac{d(p)}{w_{feather}}\right)
\end{equation}
where $d(p)$ is the distance from pixel $p$ to the mask boundary.

\textbf{Multi-Resolution Pyramid Blending:} Blends Laplacian pyramids at each resolution level:
\begin{equation}
L_{blend}^k = G_{mask}^k \cdot L_{src}^k + (1 - G_{mask}^k) \cdot L_{tgt}^k
\end{equation}

\textbf{Color Harmonization:} Adjusts replacement content to match scene lighting:
\begin{equation}
I_{adjusted} = \frac{\sigma_{target}}{\sigma_{source}}(I - \mu_{source}) + \mu_{target}
\end{equation}
computed in LAB color space for perceptual accuracy.

\subsection{Stage 4: Post-Processing}

\subsubsection{Enhanced Kalman Smoothing}

An enhanced 16-dimensional Kalman filter with constant velocity model is used. The state vector is ordered as $\mathbf{x} = [x_1, y_1, x_2, y_2, x_3, y_3, x_4, y_4, \dot{x}_1, \dot{y}_1, ..., \dot{x}_4, \dot{y}_4]^T$, where the first 8 elements represent the $(x,y)$ positions of the 4 billboard corners and the last 8 elements represent their velocities:

\begin{equation}
\mathbf{x}_{t} = \begin{bmatrix} I_{8} & \Delta t \cdot I_{8} \\ 0 & I_{8} \end{bmatrix} \mathbf{x}_{t-1} + \mathbf{w}
\end{equation}
\begin{equation}
\mathbf{z}_{t} = \begin{bmatrix} I_{8} & 0 \end{bmatrix} \mathbf{x}_{t} + \mathbf{v}
\end{equation}

where $I_8$ is the $8 \times 8$ identity matrix, and $\Delta t = 1$ for consecutive frames. The filter includes adaptive noise scaling based on innovation magnitude and automatic reset on large displacements ($> 200$ pixels).

\subsubsection{IIR Multi-Stage Smoothing}

As an alternative, cascaded Infinite Impulse Response (IIR) filtering is applied:
\begin{equation}
\hat{p}_t = \alpha \cdot p_t + (1 - \alpha) \cdot \hat{p}_{t-1}
\end{equation}
with $\alpha = 0.3$ applied across 2-3 stages for aggressive smoothing without excessive lag.

\subsection{Datasets and Experimental Setup}

\subsubsection{Detection Dataset}

A large-scale detection dataset was constructed comprising 3,798 images sourced from multiple public repositories (Roboflow). The dataset is split into 3,038 training, 549 validation, and 211 testing images. It captures a wide variety of billboard types, sizes, and environmental conditions. To enhance model robustness, extensive data augmentation was applied during training, including HSV color-space transformations (Hue: $\pm0.015$, Saturation: $\pm0.7$, Value: $\pm0.4$), geometric distortions (rotation $\pm15^\circ$, shear $\pm5^\circ$, perspective $0.001$), and advanced regularization techniques such as Mosaic ($p=1.0$), MixUp ($p=0.15$), and Random Erasing ($p=0.4$).

\subsubsection{Segmentation Dataset}

For segmentation tasks, a dedicated dataset of 485 images annotated with precise polygon masks was utilized. This dataset combines samples from billboard video frames and sports broadcasts to represent complex real-world scenarios. Similar augmentation strategies were employed, with the addition of Copy-Paste augmentation ($p=0.3$) to improve instance segmentation performance.

\subsubsection{Video Test Set}

Four real-world videos were used for testing purposes, each exhibiting distinct characteristics. The first video contains a stationary billboard with dynamic background changes. The second video features a billboard occupying the majority of the frame with slow camera movement, as shown in Figure \ref{fig:qualitative}. The third video depicts a zoom-in sequence on a distant billboard, testing detection performance across varying scales. The fourth video contains a digital LED billboard with constantly changing content (Figure \ref{fig:qualitative2}), presenting additional challenges for tracking algorithms that rely on appearance consistency across frames. These diverse scenarios facilitated a detailed performance comparison of both the detection models and tracking algorithms.


%=============================================================================
% IV. EXPERIMENTS AND RESULTS
%=============================================================================
\section{Experiments and Results}

\subsection{Detection Results}

Table \ref{tab:detection} presents detection performance across model variants. YOLOv8m achieves the best AP50 of 0.736, outperforming both smaller variants and zero-shot YOLO-Worldv2.

\begin{table}[htbp]
\caption{Billboard Object Detection Performance}
\label{tab:detection}
\centering
\begin{tabular}{lcccc}
\toprule
\textbf{Model} & \textbf{P} & \textbf{R} & \textbf{AP50} & \textbf{AP50-95} \\
\midrule
YOLOv8n & 0.752 & 0.643 & 0.723 & 0.473 \\
YOLOv8s & 0.787 & 0.612 & 0.731 & 0.479 \\
YOLOv8m & 0.818 & 0.600 & \textbf{0.736} & \textbf{0.480} \\
YOLO-Worldv2 (Zero-shot) & 0.330 & 0.690 & 0.598 & 0.408 \\
\bottomrule
\end{tabular}
\end{table}

The zero-shot YOLO-Worldv2 achieves higher recall (0.690) but significantly lower precision (0.330), resulting in many false positives. Fine-tuned YOLOv8m outperforms zero-shot by +0.138 AP50.

\subsubsection{Key Findings in Detection}
\begin{itemize}
    \item \textbf{Impact of Fine-tuning:} Fine-tuned models significantly outperformed zero-shot YOLO-Worldv2, with YOLOv8m achieving a \textbf{+0.138 higher AP50}, validating the necessity of domain-specific training for billboard localization.
    \item \textbf{Precision-Recall Trade-off:} While YOLO-Worldv2 exhibited the highest recall (\textbf{0.690}), its low precision (\textbf{0.330}) resulted in excessive false positives; whereas YOLOv8m maintained a superior balance with \textbf{0.818 precision}.
    \item \textbf{Scalability:} Detection performance scaled with model size, where YOLOv8m provided the best overall accuracy (\textbf{0.736 AP50}), suggesting that detection is less sensitive to overfitting than pixel-level segmentation on this dataset.
\end{itemize}


\subsection{Segmentation Results}

Table \ref{tab:segmentation} compares segmentation approaches. Fine-tuned YOLOv8n-seg achieves the best performance with 0.672 AP50.

\begin{table}[htbp]
\caption{Billboard Segmentation Performance}
\label{tab:segmentation}
\centering
\begin{tabular}{lcccc}
\toprule
\textbf{Model} & \textbf{P} & \textbf{R} & \textbf{AP50} & \textbf{AP50-95} \\
\midrule
YOLOv8n-seg & 0.786 & \textbf{0.684} & \textbf{0.672} & 0.542 \\
YOLOv8s-seg & 0.819 & 0.512 & 0.586 & 0.442 \\
YOLOv8m-seg & 0.693 & 0.605 & 0.636 & 0.462 \\
Mask R-CNN & \textbf{0.867} & 0.605 & 0.626 & \textbf{0.531} \\
YOLOv8+SAM2 & 0.407 & 0.512 & 0.360 & 0.284 \\
\bottomrule
\end{tabular}
\end{table}

\subsubsection{Key Findings in Segmentation}
\begin{itemize}
    \item \textbf{Optimal Architecture:} YOLOv8n-seg achieves the highest AP50 (\textbf{0.672}) with the best recall (\textbf{0.684}), demonstrating that lightweight models generalize effectively on domain-specific datasets with limited samples (485 images).
    \item \textbf{Mask R-CNN Performance:} After proper AP calculation using PR curves, Mask R-CNN shows strong results with the highest precision (\textbf{0.867}) and competitive AP50-95 (\textbf{0.531}). This indicates that two-stage detectors can achieve high-quality mask boundaries when properly trained.
    \item \textbf{Boundary and Mask Fidelity:} YOLOv8n-seg leads in detection-weighted accuracy (AP50: 0.672), while Mask R-CNN excels in stricter IoU thresholds (AP50-95: 0.531), suggesting complementary strengths for different application requirements.
    \item \textbf{YOLOv8+SAM2 Pipeline:} The detection+segmentation pipeline shows lower benchmark scores (AP50: 0.360) due to error propagation from detection to segmentation. However, qualitative testing reveals that SAM2 produces high-quality masks when provided with accurate bounding boxes, making it valuable for video applications where per-frame detection is feasible.
\end{itemize}

\subsection{Tracking Results}

Table \ref{tab:tracking} presents tracking algorithm performance across the video test set. Evaluation metrics include:

\begin{itemize}
    \item \textbf{Mean IoU:} Intersection over Union between tracked and reference masks.
    \item \textbf{Success@0.5:} Percentage of frames with IoU $> 0.5$.
    \item \textbf{Failures:} Frames with IoU $< 0.5$ indicating significant tracking degradation.
    \item \textbf{FPS:} Processing speed benchmarked on an NVIDIA RTX 4070 12GB GPU with AMD Ryzen 5 5600X 6-core processor.
\end{itemize}

\begin{equation}
\text{IoU} = \frac{|M_{pred} \cap M_{gt}|}{|M_{pred} \cup M_{gt}|}
\end{equation}

\begin{table}[htbp]
\caption{Tracking Algorithm Performance (YOLOv8m-det + SAM2, 379 frames)}
\label{tab:tracking}
\centering
\begin{tabular}{lccccc}
\toprule
\textbf{Tracker} & \textbf{Mean IoU} & \textbf{S@0.5} & \textbf{S@0.7} & \textbf{Fail} & \textbf{FPS} \\
\midrule
PlanarKalman & \textbf{0.915} & \textbf{99.7\%} & \textbf{96.5\%} & \textbf{1} & \textbf{283.7} \\
FeatureHomo & 0.907 & 96.2\% & 92.7\% & 14 & 2.9 \\
HybridFlow & 0.906 & 97.6\% & 88.6\% & 9 & 35.7 \\
SAM2 & 0.859 & 96.2\% & 91.3\% & 14 & 8.6 \\
ECCHomo & 0.684 & 79.8\% & 38.0\% & 76 & 1.9 \\
OpticalFlow & 0.577 & 63.1\% & 59.6\% & 143 & 32.8 \\
\bottomrule
\end{tabular}
\end{table}

\subsubsection{Key Findings in Tracking}
\begin{itemize}
    \item \textbf{Best Overall:} PlanarKalmanTracker achieves the highest mean IoU (\textbf{0.915}) while simultaneously delivering the fastest processing speed (\textbf{283.7 FPS}), with only 1 tracking failure across 379 frames. This makes it ideal for real-time billboard replacement applications.
    \item \textbf{Feature-Based Accuracy:} FeatureHomographyTracker (SIFT-based) achieves 0.907 mean IoU with excellent boundary precision (Median IoU: 0.953), though at significantly lower speed (2.9 FPS) due to computational overhead of feature extraction and matching.
    \item \textbf{Speed-Accuracy Balance:} HybridFlowTracker provides an excellent trade-off with 0.906 mean IoU at 35.7 FPS, combining dense optical flow with feature correspondence for robust tracking across camera motion.
    \item \textbf{Neural Network Tracking:} SAM2Tracker maintains consistent performance (0.859 mean IoU) using memory-based attention mechanisms, with moderate computational cost (8.6 FPS). Its uniform behavior across diverse video conditions makes it suitable when consistency is prioritized over peak accuracy.
    \item \textbf{Classical Methods:} ECCHomography and AdaptiveOpticalFlow showed degraded performance on videos with significant motion or texture-poor billboard surfaces, highlighting the need for hybrid or learning-based approaches in challenging conditions.
\end{itemize}

\subsection{Qualitative Results}

Figure \ref{fig:qualitative} shows example replacement results. The pipeline successfully handles perspective changes, maintains temporal consistency, and produces visually coherent output.

\begin{figure}[htbp]
\centerline{\includegraphics[width=\columnwidth]{comparison.png}}
\caption{Comparison of the original video frame (top) and the automated billboard replacement result (bottom).}
\label{fig:qualitative}
\end{figure}

\begin{figure}[htbp]
\centerline{\includegraphics[width=\columnwidth]{comparison2.png}}
\caption{Additional replacement example demonstrating the pipeline's robustness across different viewing angles and lighting conditions.}
\label{fig:qualitative2}
\end{figure}

%=============================================================================
% V. DISCUSSION AND FUTURE WORK
%=============================================================================
\section{Discussion and Future Work}

\subsection{Fine-Tuned vs Zero-Shot Detection}

Experiments demonstrate that fine-tuned models significantly outperform zero-shot approaches for domain-specific billboard detection. YOLOv8m achieves 0.736 AP50 compared to 0.598 for YOLO-Worldv2, a +0.138 improvement. However, zero-shot models maintain value for rapid prototyping or scenarios where training data is unavailable.

\subsection{Tracking vs Per-Frame Segmentation}

While the tracking benchmark results (Table \ref{tab:tracking}) demonstrate that algorithms such as PlanarKalmanTracker achieve high IoU scores (0.915) with excellent processing speeds (283.7 FPS), qualitative evaluation on the test video sequences revealed an important insight: performing detection and segmentation on every frame consistently produced superior visual quality compared to tracking-based mask propagation.

The primary reasons for this observation include:

\begin{itemize}
    \item \textbf{Drift Accumulation:} Tracking algorithms, even with periodic keyframe re-detection, accumulate small errors over time that manifest as jittery or misaligned mask boundaries.
    \item \textbf{Motion Robustness:} Per-frame detection+segmentation handles rapid camera motion and viewpoint changes more gracefully, as each frame is processed independently without relying on temporal correspondence.
    \item \textbf{SAM2 Performance Gap:} Interestingly, while SAM2 showed relatively lower performance in the static segmentation benchmark (AP50: 0.307 in Table \ref{tab:segmentation}), it demonstrated significantly better results when used in the detect+segment pipeline with YOLOv8m-provided bounding boxes. This suggests that SAM2's strength lies in its prompt-based segmentation capability rather than standalone detection, making the YOLOv8+SAM2 combination particularly effective for video applications.
\end{itemize}

These findings suggest that for applications prioritizing visual quality over processing speed, per-frame detection+segmentation may be preferable despite the computational overhead. Hybrid approaches that use tracking for intermediate frames while performing full segmentation at regular intervals offer a practical compromise.

\subsection{Current Limitations}

Several limitations affect pipeline performance:

\begin{itemize}
    \item \textbf{Quadrilateral Approximation:} Our replacement method uses 4-corner homography, which assumes billboards are rectangular. Non-rectangular or curved billboards require mask-based image warping (e.g., thin-plate spline) to properly fit replacement content to arbitrary mask shapes.
    \item \textbf{Extreme Viewing Angles:} Billboards viewed at acute angles suffer from resolution loss after perspective transformation.
    \item \textbf{Occlusion Handling:} Partial occlusions (e.g., pedestrians, vehicles) cause tracking failures and visual artifacts.
    \item \textbf{Dynamic Content:} LED billboards with changing content confuse tracking algorithms that assume static appearance.
    \item \textbf{Computational Cost:} Transformer-based segmentation models (SAM2) require GPU acceleration with adequate VRAM (12GB+ recommended).
\end{itemize}

\subsection{Future Improvements}

Several directions could enhance pipeline capabilities:

\subsubsection{Diffusion-Based Image Replacement}

The current implementation uses OpenCV's perspective transformation and alpha blending for content replacement. While effective for static image overlays, this approach has limitations such as hard edges at mask boundaries and lighting inconsistencies. Future versions could leverage diffusion models like Stable Diffusion for inpainting, enabling seamless blending with scene context. Text-to-image capabilities would allow generating novel billboard content from prompts without pre-existing assets. The main trade-off is computational cost, as diffusion-based methods require significantly more processing time compared to geometric transformation.

\subsubsection{Multi-Billboard Tracking}

The current pipeline processes a single billboard per video. Extending to simultaneous tracking of multiple billboards would require association algorithms such as SORT or DeepSORT to maintain identity across frames and handle billboard entries/exits from the scene.

\subsubsection{Occlusion-Aware Replacement}

Partial occlusions by pedestrians or vehicles currently cause visual artifacts in the replaced content. Leveraging depth estimation models (MiDaS, ZoeDepth) or instance segmentation could enable proper layering of occluding objects over the replaced billboard content, significantly improving visual realism in complex scenes.

%=============================================================================
% VI. CONCLUSION
%=============================================================================
\section{Conclusion}

This paper presented an automated pipeline for billboard detection, tracking, and replacement in video content. Comprehensive evaluation demonstrates that fine-tuned YOLOv8 models significantly outperform zero-shot approaches for domain-specific detection (0.736 vs. 0.598 AP50). For tracking, PlanarKalmanTracker achieves both the highest accuracy (0.915 mean IoU) and fastest processing speed (283.7 FPS), with only 1 tracking failure across 379 frames, making it ideal for real-time billboard replacement applications.

The integrated pipeline successfully automates a traditionally manual and expensive post-production task, enabling scalable content localization and dynamic advertising applications. Future work will focus on handling occlusions, improving boundary blending through generative models, and optimizing for real-time deployment.

%=============================================================================
% REFERENCES
%=============================================================================
\begin{thebibliography}{00}
\bibitem{bhargavi2023zero} D. Bhargavi, K. Sindwani, and S. Gholami, ``Zero-shot virtual product placement in videos,'' in \emph{ACM IMX}, 2023.
\bibitem{bradski2000opencv} G. Bradski, ``The OpenCV Library,'' \emph{Dr. Dobb's Journal of Software Tools}, 2000.
\bibitem{kalman1960new} R. E. Kalman, ``A New Approach to Linear Filtering and Prediction Problems,'' \emph{Journal of Basic Engineering}, vol. 82, no. 1, pp. 35--45, 1960.
\bibitem{cheng2023cutie} H. K. Cheng, S. W. Oh, B. Price, J.-Y. Lee, and A. Schwing, ``Putting the object back into video object segmentation,'' 2024.
\bibitem{cheng2022xmem} H. K. Cheng and A. G. Schwing, ``XMem: Long-Term Video Object Segmentation with an Atkinson-Shiffrin Memory Model,'' in \emph{ECCV}, 2022.
\bibitem{ultralytics2021} G. Jocher, A. Chaurasia, and J. Qiu, ``Ultralytics YOLO,'' Jan. 2023.
\bibitem{kirillov2023segment} A. Kirillov et al., ``Segment Anything,'' in \emph{ICCV}, 2023.
\bibitem{redmon2016yolo} J. Redmon, S. K. Divvala, R. B. Girshick, and A. Farhadi, ``You only look once: Unified, real-time object detection,'' \emph{CoRR}, abs/1506.02640, 2015.
\bibitem{rombach2022high} R. Rombach et al., ``High-Resolution Image Synthesis with Latent Diffusion Models,'' in \emph{CVPR}, pp. 10684–10695, 2022.
\bibitem{ravi2024sam2} N. Ravi, V. Gabeur, Y.-T. Hu, R. Hu, C. Ryali, T. Ma, H. Khedr, R. Rädle, C. Rolland, L. Gustafson, E. Mintun, J. Pan, K. V. Alwala, N. Carion, C.-Y. Wu, R. Girshick, P. Dollár, and C. Feichtenhofer, ``SAM 2: Segment Anything in Images and Videos,'' arXiv preprint arXiv:2408.00714, 2024.
\bibitem{sharma2024diffusion} A. Sharma, M. Weiss, P. Navarathna, A. Mahdavi-Amiri, and Y. Aksoy, ``Automated Virtual Product Placement and Assessment in Images using Diffusion Models,'' arXiv preprint arXiv:2407.01117, 2024.
\bibitem{yang2023track} J. Yang et al., ``Track Anything: Segment Anything Meets Videos,'' arXiv preprint arXiv:2304.11968, 2023.
\bibitem{ye2023ipadapter} H. Ye, J. Zhang, S. Liu, X. Han, and W. Yang, ``IP-Adapter: Text Compatible Image Prompt Adapter for Text-to-Image Diffusion Models,'' 2023.
\end{thebibliography}
\end{document}
